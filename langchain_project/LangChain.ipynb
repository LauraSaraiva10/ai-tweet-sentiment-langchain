{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76b45edc-2e36-4970-8d64-93ae170361eb",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40bf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf6a1976-81c7-44a6-9afb-cdc367f7bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a69f61c",
   "metadata": {},
   "source": [
    "### Set Up OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f5f5e-f8c0-4ec7-acf4-eed6fc2a5bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caa36c89-f113-4a0a-b1bc-2405c5f1165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key=os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4088e63d-3c23-4f4e-8122-d8d8dc0993fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85434064-2c7e-4f72-88c7-1e2f2b9c66da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import(\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ae109",
   "metadata": {},
   "source": [
    "This section demonstrates how to use LangChain to pass a sequence of messages between the system and user, using ChatOpenAI for interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47eb6a64-33e4-4cd5-b6a5-21b7e420f283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Large-language models are a type of artificial intelligence technology that has gained significant attention in the field of natural language processing. These models are built using deep learning techniques and are trained on vast amounts of text data to understand and generate human-like language. By leveraging massive datasets and powerful computing resources, large-language models are capable of generating coherent text, completing sentences, and even engaging in conversations. Some popular examples of large-language models include OpenAI's GPT-3 and Google's BERT. These models have\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 100, 'prompt_tokens': 27, 'total_tokens': 127, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'length', 'logprobs': None} id='run-7137da0d-604a-4176-9e88-df15d6d77720-0'\n"
     ]
    }
   ],
   "source": [
    "response = ChatOpenAI(model_name = 'gpt-3.5-turbo', temperature=1, max_tokens=100)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content='You are an expert in data science.'),\n",
    "    HumanMessage(content='Write a paragraph about large-language models.')\n",
    "]\n",
    "\n",
    "print(response(messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3040dcde-7cfc-4b5a-9767-dd7a12134220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578f9513",
   "metadata": {},
   "source": [
    "LangChain's PromptTemplate is used to create a dynamic template for generating code based on user input (such as language and task). Here's an example of generating Python code for a task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ecd4f1e-4410-4b96-9472-9f19632d0756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "def calculate_average(numbers):\n",
      "    sum = 0\n",
      "    for num in numbers:\n",
      "        sum += num\n",
      "    average = sum/len(numbers)\n",
      "    return average\n",
      "\n",
      "numbers = [5, 10, 15, 20, 25]\n",
      "print(\"The average of the numbers is:\", calculate_average(numbers))\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(model_name='gpt-3.5-turbo-instruct', temperature=0.7, max_tokens=100)\n",
    "\n",
    "template = '''\n",
    "You are an expert coder\n",
    "Write a program in {language} to {task}\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['language', 'task'],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "response = llm(prompt.format(language='Python', task='function to calculate an average'))\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74d3419f-518e-4853-bfda-44ba68d62b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6749056",
   "metadata": {},
   "source": [
    "In this section, LangChain's SimpleSequentialChain is used to combine tasks. The chain generates a mathematical equation and then provides an explanation for the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c2daf35-f2dc-44f2-8ea1-c20db341d0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "The Pythagorean Theorem is represented by the equation a^2 + b^2 = c^2, where a and b are the lengths of the two shorter sides of a right triangle and c is the length of the hypotenuse. \u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "This python code is a simple implementation of the Pythagorean Theorem. It takes in user input for the lengths of the two shorter sides of a right triangle, a and b, and calculates the length of the hypotenuse, c, using the equation c = sqrt(a^2 + b^2). The \"sqrt\" function is used to find the square root of the sum of a^2 and b^2, representing the length of the hypotenuse. The result is\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Pythagorean Theorem', 'output': '\\nThis python code is a simple implementation of the Pythagorean Theorem. It takes in user input for the lengths of the two shorter sides of a right triangle, a and b, and calculates the length of the hypotenuse, c, using the equation c = sqrt(a^2 + b^2). The \"sqrt\" function is used to find the square root of the sum of a^2 and b^2, representing the length of the hypotenuse. The result is'}\n"
     ]
    }
   ],
   "source": [
    "llm1 = OpenAI(model_name='gpt-3.5-turbo-instruct', temperature=0.7, max_tokens=100)\n",
    "\n",
    "prompt1 = PromptTemplate(\n",
    "    input_variables=['description'],\n",
    "    template='''\n",
    "    You are a great mathematician.\n",
    "    Write the equation for {description}\n",
    "    '''\n",
    ")\n",
    "\n",
    "chain1 = LLMChain(llm=llm1, prompt=prompt1)\n",
    "\n",
    "llm2 = OpenAI(model_name='gpt-3.5-turbo-instruct', temperature=0.7, max_tokens=100)\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=['equation'],\n",
    "    template='''\n",
    "    Write an explanation for this python code {equation}\n",
    "    '''\n",
    ")\n",
    "\n",
    "chain2 = LLMChain(llm=llm2, prompt=prompt2)\n",
    "\n",
    "sequential_chain = SimpleSequentialChain(chains=[chain1, chain2], verbose=True)\n",
    "\n",
    "response = sequential_chain.invoke('Pythagorean Theorem')\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
